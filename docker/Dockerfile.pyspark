# Use the Bitnami Spark base image
FROM bitnami/spark:3.2.0

# Install curl and dependencies
USER root
RUN apt-get update && \
    apt-get install -y curl && \
    apt-get clean

# Set Spark master URL (adjust as needed)
ENV SPARK_MASTER_URL=spark://spark-master:7077

# Set the work directory inside the container
WORKDIR /opt/spark

# Download Snowflake JARs (ensure the version matches your Spark version)
RUN mkdir -p /opt/spark/jars


RUN curl -o /opt/spark/jars/spark-streaming-kafka-0-10_2.12-3.2.0.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-10_2.12/3.2.0/spark-streaming-kafka-0-10_2.12-3.2.0.jar && \
    curl -o /opt/spark/jars/spark-sql-kafka-0-10_2.12-3.2.0.jar \
    https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.2.0/spark-sql-kafka-0-10_2.12-3.2.0.jar && \
    curl -o /opt/spark/jars/kafka-clients-7.4.0.jar \
    https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/7.4.0/kafka-clients-7.4.0.jar


# RUN curl -o /opt/spark/jars/snowflake-jdbc-3.13.14.jar \
#     https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.14/snowflake-jdbc-3.13.14.jar && \
#     curl -o /opt/spark/jars/spark-snowflake_2.12-2.10.0-spark_3.2.jar \
#     https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.12/2.10.0-spark_3.2/spark-snowflake_2.12-2.10.0-spark_3.2.jar


# Set PySpark submit arguments as an environment variable
ENV PYSPARK_SUBMIT_ARGS="--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell"
# ENV PYSPARK_SUBMIT_ARGS="--packages net.snowflake:spark-snowflake_2.12:2.10.0-spark_3.2,net.snowflake:snowflake-jdbc:3.13.14,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.2.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0 pyspark-shell"

# Copy application files into the container
COPY requirements.txt /opt/spark/requirements.txt
COPY pyspark_job.py /opt/spark/src/main/python/pyspark_job.py
COPY ../.env /opt/spark/.env

# COPY ../private_key.pem /opt/spark/private_key.pem
# COPY ../public_key.pem /opt/spark/public_key.pem

# Install Python dependencies
RUN pip install -r /opt/spark/requirements.txt

# Set the default command to run the PySpark job
CMD ["python", "/opt/spark/src/main/python/pyspark_job.py"]